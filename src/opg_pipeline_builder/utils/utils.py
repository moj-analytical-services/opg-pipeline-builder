import glob
import os
import re
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple, Union

import boto3
from dataengineeringutils3.s3 import (_add_slash, bucket_key_to_s3_path,
                                      s3_path_to_bucket_key)
from pyarrow import Table
from pyarrow.fs import S3FileSystem
from pyarrow.json import read_json

from .constants import aws_region


def get_last_modified(obj: dict) -> int:
    """
    Helper function to get the last modified date of an object
    from S3. Object is assumed to be an item found in boto3's
    list_objects_v2 response 'Contents' list.
    Args:
        obj (dict): Object is assumed to be an item found in boto3's
                    list_objects_v2 response 'Contents' list.
    Return:
        int:        Integer value of the objects last modified datetime
                    in S3
    """
    return int(obj["LastModified"].strftime("%s"))


def s3_copy(
    copy_args: Tuple[str, str], client: Optional[Union[object, None]] = None
) -> dict:
    """
    Function to copy objects from one S3 location to another S3 location
    Args:
        copy_args (Tuple): Tuple of S3 paths where the first item is the
                           original location of the object, and the second
                           item is the location for the copied object.
        client (optional): Boto S3 client. If not provided, one will be created.
                           This is a useful argument for multithreading.
    Retrun:
        dict: Dictionary containing the 'CopyObjectResults' dictionary of the
              clients copy_object call.
    """
    if client is None:
        client = boto3.client("s3")

    source_path, dest_path = copy_args
    dest_bucket, dest_key = s3_path_to_bucket_key(dest_path)
    source_path = source_path.replace("s3://", "")

    copy_return = client.copy_object(
        CopySource=source_path, Bucket=dest_bucket, Key=dest_key
    )

    return copy_return["CopyObjectResult"]


def s3_bulk_copy(copy_list: List[Tuple[str, str]]):
    """
    Function to copy multiple objects from a set of S3 locations to a
    a new set of S3 locations
    Args:
        copy_args (Tuple): List of tuples where the tuples first item is the
                           original location of the S3 object, and the second
                           item is the location for the copied object.
    """
    session = boto3.session.Session()
    s3_client = session.client("s3")

    max_work = min(
        max(
            32 if len(copy_list) > 80 else min(32, os.cpu_count() + 4),
            int(round(len(copy_list) / 10, 0)),
        ),
        1000,
    )

    with ThreadPoolExecutor(max_workers=max_work) as executor:
        copy_futures = [
            executor.submit(s3_copy, copy_task, s3_client) for copy_task in copy_list
        ]

    return copy_futures


def remove_lint_filestamp(
    filename: str, file_ext: Optional[str] = ".snappy.parquet"
) -> str:
    """
    Removes a filestamp generated by data_linter as part of its linting process
    from a filename/path.
    Args:
        filename (str):           filename of object that has been linted.
        file_ext (optional, str): file extension of the linted object.
                                  Assumes '.snappy.parquet' if none provided.
    Return:
        str: original filename of object, prior to linting.
    """
    regex_pattern = f"""-[0-9]{{1,2}}-[0-9]{{{10}}}{file_ext}$"""
    stamp_str = re.search(regex_pattern, filename)

    if stamp_str is None:
        raise ValueError("Filename is not in the expected format.")

    ofname = filename.replace(stamp_str.group(0), file_ext)

    return ofname


def extract_mojap_partition(
    filepath: str, timestamp_partition_name: Optional[str] = "mojap_file_land_timestamp"
) -> str:
    """
    Extracts the mojap_file_land_timestamp hive partition
    from a filepath, if one exists.
    Args:
        filepath (str): Filepath which contains hive partition.
        timestamp_partition_name (optional - str):
            The name of the partition, as say supplied to data_linter.
    Return:
        str: Hive partition of the format
             'mojap_file_land_timestamp=XXXXXXXXXX'
    """
    regex_pattern = f"""{timestamp_partition_name}=[0-9]{{{10}}}"""
    stamp_str = re.search(regex_pattern, filepath)

    if stamp_str is None:
        raise ValueError("Filepath does not contain the expected MoJ AP Hive partition")

    partition = stamp_str[0]

    return partition


def extract_mojap_timestamp(
    partition: str,
    timestamp_partition_name: Optional[str] = "mojap_file_land_timestamp",
) -> int:
    """
    Extracts timestamp from a mojap hive partition
    Args:
        partition (str): Hive partition of the format
            'mojap_file_land_timestamp=XXXXXXXXXX'
        timestamp_partition_name (optional - str):
            The name of the partition, as say supplied to
            data_linter.
    Return:
        int: Partition timestamp as integer
    """
    regex = f"^{timestamp_partition_name}="
    result = re.search(regex, partition)

    if result is None:
        raise ValueError(f"{partition} is not in the correct format")

    timestamp_str = partition.replace(result[0], "")

    check_regex = "^[0-9]{10}$"
    check_result = re.search(check_regex, timestamp_str)

    if check_result is None:
        raise ValueError(f"{partition} is not in the correct format")

    timestamp = int(timestamp_str)

    return timestamp


def get_modified_filepaths_from_s3_folder(
    s3_folder_path: str,
    file_extension: Optional[Union[str, None]] = None,
    exclude_zero_byte_files: Optional[bool] = True,
    modified_after: Optional[Union[datetime, None]] = None,
    modified_before: Optional[Union[datetime, None]] = None,
) -> List[str]:
    """
    Get a list of filepaths from a bucket. If extension is set to a string
    then only return files with that extension otherwise if set to None (default)
    all filepaths are returned.
    Args:
        s3_folder_path (str):                        "s3://...."
        extension (optional - str, None):            File extension, e.g. .json
        exclude_zero_byte_files (optional - bool):   Whether to filter out results
                                                     of zero size. Default set to True
        modified_after (optional - datetime, None):  Only list files modified after
                                                     the given datetime (UTC)
        modified_before (optional - datetime, None): Only list files modified before
                                                     the given datetime (UTC)
    Return:
        List[str]: A list of full s3 paths that were in the given s3 folder path
    """

    s3_resource = boto3.resource("s3")

    if file_extension is not None:
        if file_extension[0] != ".":
            file_extension = "." + file_extension

    # This guarantees that the path the user has given is really a 'folder'.
    s3_folder_path = _add_slash(s3_folder_path)

    bucket, key = s3_path_to_bucket_key(s3_folder_path)

    s3b = s3_resource.Bucket(bucket)
    obs = s3b.objects.filter(Prefix=key)

    if file_extension is not None:
        obs = [o for o in obs if o.key.endswith(file_extension)]

    if exclude_zero_byte_files:
        obs = [o for o in obs if o.size != 0]

    if modified_after is not None:
        obs = [o for o in obs if o.last_modified > modified_after]

    if modified_before is not None:
        obs = [o for o in obs if o.last_modified < modified_before]

    ob_keys = [o.key for o in obs]

    paths = sorted([bucket_key_to_s3_path(bucket, o) for o in ob_keys])

    return paths


def list_configs() -> List[str]:
    """
    Returns databases with configs in the configs folder in the
    projects root directory.
    Return:
        List[str]: List of databases with configs
    """
    config_yml_files = glob.glob(os.path.join("configs", "*yml"))

    config_names = [Path(f).stem for f in config_yml_files]

    return config_names


def _get_file_result(max_existing_ts: int, new_ts: int, one_a_day: bool) -> bool:
    """
    Helper function for checking whether to retrieve
    a new file.
    Args:
        max_existing_ts (int): Latest timestamp for
                               existing data
        new_ts (int):          Timestamp for new data
        one_a_day (bool):      True - only one file per
                               day
    Return:
        bool: Whether to get new file
    """
    if not one_a_day:
        if max_existing_ts < new_ts:
            get_file = True
        else:
            get_file = False
    else:
        max_date = datetime.fromtimestamp(max_existing_ts).date()
        new_file_date = datetime.fromtimestamp(new_ts).date()
        if max_date < new_file_date:
            get_file = True
        else:
            get_file = False

    return get_file


def check_s3_for_existing_timestamp_file(
    existing_data: List[str],
    new_file: str,
    one_a_day: Optional[bool] = True,
    filename_regex: Optional[str] = (
        "^([a-zA-z/?]+)([_-]{1})([0-9]{10})" "([-0-9]{3})?([0-9]{10})?([.][a-z.]+)$"
    ),
) -> bool:
    """
    returns True if file does not exist for a given timestamp or
    day of timestamp and indicates pipeline should get the file.
    Args:
        existing_data (List[str]):   List of S3 paths for exisiting data
        new_file (str):              S3 path for new file to process
        one_a_day (optional - bool): True - only one file per
                                     day
        filename_regex:              Regex for extracting timestamp from
                                     file
    Return:
        bool: Whether to process file
    """
    try:
        # get max timestamp in raw_hist for given table
        ts = [re.search(filename_regex, Path(i).name).group(3) for i in existing_data]
    except AttributeError:
        raise ValueError(
            """a file timestamp in raw_hist is not
               in the expected format"""
        )

    try:
        max_ts = int(max(ts))
    except Exception:
        max_ts = 0

    try:
        new_file_timestamp = re.search(filename_regex, Path(new_file).name).group(3)
    except AttributeError:
        raise ValueError(
            f"""the new filename, {new_file}, is not
               in the expected format"""
        )

    if not isinstance(new_file_timestamp, int) and not len(new_file_timestamp) == 10:
        raise ValueError("wrong format for new timestamp")

    get_file = _get_file_result(max_ts, int(new_file_timestamp), one_a_day)

    return get_file


def pa_read_json_from_s3(s3_path: str) -> Table:
    """
    Reads a JSON file from S3 using PyArrow

    Args:
        s3_path (str): S3 path to JSON file
    """
    s3 = S3FileSystem(region=aws_region)

    b, k = s3_path_to_bucket_key(s3_path)
    pa_pth = os.path.join(b, k)

    with s3.open_input_file(pa_pth) as file:
        json_pa_data = read_json(file)

    return json_pa_data


def do_nothing(*args, **kwargs): ...
